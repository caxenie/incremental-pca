{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical Machine Learning on Evolving Non-stationary Data Streams\n",
    "\n",
    "## Intuition, Formalism and Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stream Processing\n",
    "\n",
    "Given a sequence of data (a stream), a series of operations (functions) is applied to each element in the stream, in a declarative way, we specify what we want to achieve and not how [Bifet, 2010].\n",
    "![](streaming-intuition.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stream processing\n",
    "\n",
    "Data __Stream Learning__ is more challenging than __batch or offline learning__ [Bifet, 2010]:\n",
    "* The amount of data is __extremely large__, potentially infinite - __impossible to store__ it all. \n",
    "* Only a __small summary__ can be computed and stored, and the rest is discarded - unfeasible to go over it for processing.\n",
    "* The __speed of arrival is high__, so that each particular element has to be processed in __real time__, and then discarded.\n",
    "* The __distribution generating the items__ can __change over time__. \n",
    "* __Data from the past__ may become __irrelevant (or even harmful)__for the current summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stream processing\n",
    "\n",
    "![](ml-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond i.i.d\n",
    "\n",
    "* __Traditional machine learning and data mining__ assume the current observed data and the future data are assumed to be i.i.d\n",
    "* __Data samples__, past and current data samples do not affect the probability for future ones. \n",
    "\n",
    "* Applications like:\n",
    "    * web mining, \n",
    "    * social networks, \n",
    "    * network monitoring, \n",
    "    * sensor networks, \n",
    "    * telecommunications, \n",
    "    * financial forecasting, etc.\n",
    "* Data samples arrive __continuously__, __online__ through unlimited streams often at __high speed, over time__\n",
    "* The __process__ generating these data streams __may evolve over time__ (evolving, nonstationarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond i.i.d\n",
    "\n",
    "In order to deal with evolving data streams, the model learnt from the streaming data must capture up-to-date trends and transient patterns in the stream. \n",
    "\n",
    "Updating the model by incorporating new examples, we must also eliminate the effects of outdated examples representing outdated concepts through one-pass.\n",
    "\n",
    "Types of change\n",
    "![](changes-types.png)\n",
    "\n",
    "Typical changes in classification\n",
    "![](changes-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond i.i.d\n",
    "\n",
    "When __training and test data__ follow __different probability distributions__, but the __conditional distributions of output__ values given input points are __unchanged__, is called the __covariate shift__ [Sugiyama et al., 2012]. \n",
    "\n",
    "This means that the __target function__ we want to learn is __unchanged between the training phase and the test phase__, but the __distributions__ of input points are __different__ for training and test data.\n",
    "\n",
    "![](covariate-shift.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beyond i.i.d\n",
    "\n",
    "The key idea of __covariate shift adaptation__ is to (softly) __choose informative training samples__ in a systematic way, by considering the __importance__ of each training sample in the prediction of test output values, namely the ratio\n",
    "\n",
    "$$ \\frac{p_{te}(x_i^{tr})}{p_{tr}(x_i^{tr})}$$\n",
    "\n",
    "Basically, __the expectation__ of a function $f$ (i.e. regression) over $x_{te}$ can be computed by the __importance-weighted expectation__ of the function over $x_{tr}$. Thus, the __difference of distributions__ can be systematically __adjusted__ by importance weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Concept drift\n",
    "\n",
    "Data generated by phenomena in __nonstationary environments__ are characterized by: \n",
    "\n",
    "* potentially unlimited size;\n",
    "* sequential access to data samples (i.e. once an observation has been processed, it cannot be retrieved);\n",
    "* unpredictable, dependent, and not identical distributed observations.\n",
    "\n",
    "__Learning from streams of nonstationary data__ lives under the following constraints[Sayed-Mouchaweh, 2016]:\n",
    "* __Random access to observations__ is __not feasible__, or it has high costs (i.e. dataset not a priori available or too large).\n",
    "* __Memory is small__ with respect to the size of data.\n",
    "* Data __distribution__ generating the data may __evolve over time__. This is also known as __concept drift__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Incremental learning\n",
    "\n",
    "Unlike __conventional machine learning__, the __data flow__ targeted by __incremental learning__ becomes available __continuously__ over time and needs to be __processed in a single pass__.\n",
    "\n",
    "The inherent __challenges__ here are: \n",
    "\n",
    "* __data availability__ \n",
    "* __model update__ \n",
    "* __data size__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental learning\n",
    "\n",
    "The inherent __challenges__ here are: \n",
    "\n",
    "* __data availability__ - new data is received and eliminated from the window of interest as the stream evolves in time\n",
    "* __model update__ - given the dynamics of the window evolution, the updates must be performed in a single pass\n",
    "* __data size__ - precise models need large windows, yet updating the model for the entire window is costly in terms of latency and resource allocation (i.e. memory / disk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Incremental dimensionality reduction (PCA)\n",
    "\n",
    "In a __structured form__, the basic formulation, PCA follows the following steps:\n",
    "\n",
    "![](basic-pca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Incremental dimensionality reduction (PCA)\n",
    "\n",
    "![](basic-pca-problems.png)\n",
    "\n",
    "Tackle the __inherent problems in traditional PCA__ impeding it to achieve __low latency, high throughput and fixed memory/storage__:\n",
    "* Calculation of the __mean and other descriptive statistics__ as the data is available.\n",
    "* __Sorting the dominant eigenvalues__ in the rank update of the QR decomposition.\n",
    "* Calculating the __covariance matrix__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Towards incrmental PCA\n",
    "\n",
    "Incremental __calculation of the mean and other descriptive statistics__ on the datastream.\n",
    "![](streaming-mean.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Towards incrmental PCA\n",
    "\n",
    "Incremental __updates depending on counts (i.e. histogram)__, which contain sorted eigenvalues.\n",
    "![](streaming-histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Towards incrmental PCA\n",
    "\n",
    "Incremental __estimation of the covariance matrix__, as __neural synaptic weights__ converge to the eigenvectors (unique set of __optimal weights__ and __uncorrelated outputs__) [Axenie et al., 2019]\n",
    "![](streaming-neural-covariance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Towards incrmental PCA\n",
    "\n",
    "Converge from an initially __random set of synaptic weights__ to the __eigenvectors of the input autocorrelation__ in the eigenvalues order __minimizing the linear reconstruction (i.e. using Linear Least Squares)__. \n",
    "![](streaming-lls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example implementation\n",
    "\n",
    "### Multi-class classification task (fault identification in predictive maintenance [Axenie et al., 2019])\n",
    "\n",
    "We used a __real-world stream__ with sensory readings from a coal coke prediction production line data:\n",
    "* 1 preheater temperature sensor, \n",
    "* 2 briquetting temperature sensors, \n",
    "* 2 cooker temperature sensors, \n",
    "* 2 coke quencher temperature sensors, \n",
    "* 2 coke transport system temperature sensors,\n",
    "* 2 blast furnace temperature sensors. \n",
    "\n",
    "![](coke.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example implementation\n",
    "\n",
    "### Multi-class classification task (fault identification in predictive maintenance [Axenie et al., 2019])\n",
    "\n",
    "__Goal__: __identify faults__ in the production line by __querying the eigenvalues and eigenvectors__ to extract the normal and faulty operation configuration __prior to a multi-class classifier__. \n",
    "\n",
    "The datastream:\n",
    "* 2M incoming events at 40 kHz. \n",
    "* eigenvalues of the input $X$ are close to the class labels (i.e. $1, 2, ..., d$)\n",
    "* eigenvectors are close to the canonical basis of $R^d$, where $d$ is the number of principal components to extract\n",
    "* class number for the multi-class classification task (i.e. 10 classes, 9 faults and 1 normal).\n",
    "\n",
    "![](experimental-setup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA vs. Incremental PCA (Latency Analysis)\n",
    "\n",
    "![](latency-analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA vs. Incremental PCA (Throughput Analysis)\n",
    "\n",
    "![](throughput-analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA vs. Incremental PCA (Accuracy Analysis)\n",
    "\n",
    "![](accuracy-analysis.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "* __low-latency__ (1-ms level), __high-throughput__ (Kevents/s) computation and __learning on datastreams__,\n",
    "\n",
    "* __incremental PCA model__ and leveraging __stream dimensionality reduction__ on a distributed system,\n",
    "\n",
    "* computation of __statistical features__ and __neural learning rules__ \n",
    "\n",
    "* __guaranteeing limited or programmable resource__ allocation (i.e. memory and disk),\n",
    "\n",
    "* validated in __predictive maintenance__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "[Bifet, 2010] Albert Bifet - Adaptive Stream Mining Pattern Learning and Mining from Evolving Data Streams, IOS Press, 2010.\n",
    "\n",
    "[Sugiyama et al., 2012] Masashi Sugiyama, Motoaki Kawanabe - Machine Learning in Non-Stationary Environments Introduction to Covariate Shift Adaptation-The MIT Press, 2012.\n",
    "\n",
    "[Sayed-Mouchaweh, 2016] Moamar Sayed-Mouchaweh - Learning from Data Streams in Dynamic Environments-Springer International Publishing, 2016.\n",
    "\n",
    "[Axenie et al., 2019] C. Axenie, Radu Tudoran, Stefano Bortoli, Mohamad Al Hajj Hassan, Alexander Wieder, Goetz Brasche, SPICE: Streaming PCA fault Identification and Classification Engine in Predictive Maintenance, IoT Stream Workshop, European Conf. on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2019).\n",
    "\n",
    "[Weng et al., 2003] J. Weng, Y. Zhang, and W.-S. Hwang, “Candid covariance-free incremental principal component analysis,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 8, pp. 1034–1040, 2003.\n",
    "\n",
    "[Brand, 2002] M. Brand, “Incremental singular value decomposition of uncertain data with missing values.” in ECCV (1), ser. Lecture Notes in Computer Science, A. Heyden, G. Sparr, M. Nielsen, and P. Johansen, Eds., vol. 2350. Springer, 2002, pp. 707–720."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
