{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Statistical Machine Learning on \n",
    "# Evolving Non-stationary Data Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outline\n",
    "- Stream Processing\n",
    "- Beyond i.i.d\n",
    "- Incremental Learning\n",
    "- Incremental Learning - Case Study\n",
    "- Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stream Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stream Processing\n",
    "\n",
    "Given a sequence of data (__a stream__), a series of operations (functions) is applied to each element in the stream, in a declarative way, we specify __what we want to achieve and not how__ [Bifet, 2010].\n",
    "![](./stream-animation/streamanim.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stream Processing\n",
    "\n",
    "Data __Stream Learning__ is more challenging than __batch or offline learning__ [Bifet, 2010]:\n",
    "* The amount of data is __extremely large__, potentially infinite - __impossible to store__ \n",
    "* Only a __small summary__ can be computed and stored, and the rest is discarded - unfeasible to go over it\n",
    "* The __speed of arrival is high__, so that each datum has to be processed in __real time__, and then discarded\n",
    "* The __distribution generating the items__ can __change over time__. \n",
    "* __Data from the past__ may become __irrelevant (or even harmful)__for the current summary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Stream Processing\n",
    "\n",
    "![](ml-pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond i.i.d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond i.i.d\n",
    "\n",
    "* __Traditional machine learning and data mining__ assume the current observed data and the future data are assumed to be **independent and identicically distributed (i.i.d)**\n",
    "* __Data samples__, in the past and current time do not affect the probability for future ones. \n",
    "\n",
    "\n",
    "* Yet, in applications like:\n",
    "    * web mining, \n",
    "    * social networks, \n",
    "    * network monitoring, \n",
    "    * sensor networks, \n",
    "    * telecommunications, \n",
    "    * financial forecasting, etc.\n",
    "    \n",
    "    \n",
    "* Data samples arrive __continuously__, __online__ through unlimited streams often at __high speed, over time__ [Sayed-Mouchaweh, 2016].\n",
    "* The __process__ generating these data streams __may evolve over time__ (i.e. nonstationarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond i.i.d\n",
    "\n",
    "In order to deal with evolving data streams, the __model learnt from the streaming data__ must capture up-to-date __trends__ and __transient patterns__ in the stream. \n",
    "\n",
    "__Types of change in regression__\n",
    "![](changes-types.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond i.i.d\n",
    "\n",
    "__Updating the model__ by incorporating new examples, we must also eliminate the effects of outdated examples representing outdated concepts through __one-pass__.\n",
    "\n",
    "__Typical changes in classification__\n",
    "![](changes-sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond i.i.d\n",
    "\n",
    "When __training and test data__ follow __different probability distributions__, but the __conditional distributions of output__ values given input points (i.e. __target function__) are __unchanged__, we face __covariate shift__ [Sugiyama et al., 2012]. The **model** should choose **informative training samples** by considering the **importance** of each training sample in the prediction of test output values.\n",
    "![](covariate-shift.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Incremental Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Incremental Learning\n",
    "\n",
    "Unlike __conventional machine learning__, the __data flow__ targeted by __incremental learning__ becomes available __continuously__ over time and needs to be __processed in a single pass__.\n",
    "\n",
    "The inherent __challenges__ here are: \n",
    "\n",
    "* __data availability__ \n",
    "* __model update__ \n",
    "* __data size__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Incremental Learning\n",
    "\n",
    "The inherent __challenges__ here are: \n",
    "\n",
    "* __data availability__ - new data is received and eliminated from the window of interest as the stream evolves in time\n",
    "* __model update__ - given the dynamics of the window evolution, the updates must be performed in a single pass\n",
    "* __data size__ - precise models need large windows, yet updating the model for the entire window is costly in terms of latency and resource allocation (i.e. memory / disk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Incremental Learning - Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA\n",
    "\n",
    "In a __structured form__, the basic formulation, PCA follows the following steps:\n",
    "\n",
    "![](basic-pca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA\n",
    "\n",
    "![](basic-pca-problems.png)\n",
    "\n",
    "Tackle the __inherent problems in traditional PCA__ impeding it to allow it to learn incrmentally:\n",
    "* Calculation of the __mean and other descriptive statistics__ as the data is available\n",
    "* __Sorting the dominant eigenvalues__ in the rank update of the QR decomposition\n",
    "* Calculating the __covariance matrix__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Towards incremental PCA\n",
    "\n",
    "Incremental __calculation of the mean and other descriptive statistics__ on the datastream.\n",
    "![](streaming-mean.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Towards incremental PCA\n",
    "\n",
    "Incremental __updates depending on counts (i.e. histogram)__, which contain sorted eigenvalues.\n",
    "![](streaming-histogram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Towards incremental PCA\n",
    "\n",
    "Incremental __estimation of the covariance matrix__, as __neural synaptic weights__ converge to the eigenvectors (unique set of __optimal weights__ and __uncorrelated outputs__) [Axenie et al., 2019].\n",
    "![](streaming-neural-covariance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Towards incremental PCA\n",
    "\n",
    "Converge from an initially __random set of synaptic weights__ to the __eigenvectors of the input autocorrelation__ in the eigenvalues order __minimizing the linear reconstruction (i.e. using Linear Least Squares)__. \n",
    "![](streaming-lls.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Incremental PCA - Example implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'skmultiflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1846e873dde7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib notebook'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mskmultiflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_stream\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFileStream\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mload_iris\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'skmultiflow'"
     ]
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from skmultiflow.data.file_stream import FileStream\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "from skmultiflow.data.file_stream import FileStream\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "iris = load_iris()\n",
    "stream = FileStream('./iris-dataset.csv')\n",
    "stream.prepare_for_use()\n",
    "stream.n_samples\n",
    "\n",
    "X,y = stream.next_sample(stream.n_samples)\n",
    "\n",
    "n_components = 2\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=5)\n",
    "X_ipca = ipca.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=n_components)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "\n",
    "for X_transformed, title in [(X_ipca, \"Incremental PCA\"), (X_pca, \"PCA\")]:\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    for color, i, target_name in zip(colors, [0, 1, 2], iris.target_names):\n",
    "        plt.scatter(X_transformed[y == i, 0], X_transformed[y == i, 1],\n",
    "                    color=color, lw=2, label=target_name)\n",
    "\n",
    "    if \"Incremental\" in title:\n",
    "        err = np.abs(np.abs(X_pca) - np.abs(X_ipca)).mean()\n",
    "        plt.title(title + \" of iris dataset\\nMean absolute unsigned error \"\n",
    "                  \"%.6f\" % err)\n",
    "    else:\n",
    "        plt.title(title + \" of iris dataset\")\n",
    "    plt.legend(loc=\"best\", shadow=False, scatterpoints=1)\n",
    "    plt.axis([-4, 4, -1.5, 1.5])\n",
    "    plt.box('off')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "[Bifet, 2010] Albert Bifet - Adaptive Stream Mining Pattern Learning and Mining from Evolving Data Streams, IOS Press, 2010.\n",
    "\n",
    "[Sugiyama et al., 2012] Masashi Sugiyama, Motoaki Kawanabe - Machine Learning in Non-Stationary Environments Introduction to Covariate Shift Adaptation-The MIT Press, 2012.\n",
    "\n",
    "[Sayed-Mouchaweh, 2016] Moamar Sayed-Mouchaweh - Learning from Data Streams in Dynamic Environments-Springer International Publishing, 2016.\n",
    "\n",
    "[Axenie et al., 2019] C. Axenie, Radu Tudoran, Stefano Bortoli, Mohamad Al Hajj Hassan, Alexander Wieder, Goetz Brasche, SPICE: Streaming PCA fault Identification and Classification Engine in Predictive Maintenance, IoT Stream Workshop, European Conf. on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD 2019).\n",
    "\n",
    "[Weng et al., 2003] J. Weng, Y. Zhang, and W.-S. Hwang, “Candid covariance-free incremental principal component analysis,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 25, no. 8, pp. 1034–1040, 2003.\n",
    "\n",
    "[Brand, 2002] M. Brand, “Incremental singular value decomposition of uncertain data with missing values.” in ECCV (1), ser. Lecture Notes in Computer Science, A. Heyden, G. Sparr, M. Nielsen, and P. Johansen, Eds., vol. 2350. Springer, 2002, pp. 707–720."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](moa.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volessung Notebook herunterladen\n",
    "\n",
    "<img src=\"./qrcode.png\" style=\"width = 200, height=200\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def update_incremental_mean(xbar, x, n):\n",
    "        f = 1.0 / (n + 1.0)\n",
    "        return (xbar.scale(1.0 - f)).plus(x.scale(f))\n",
    "\n",
    "def IncrementalPCA(state,x):\n",
    "        # recover state\n",
    "        lambdaVal = state.get_lambda_val()\n",
    "        Q = state.getQ()\n",
    "        ind = state.getN()\n",
    "        gamma = np.array(Q.num_cols())\n",
    "    \n",
    "        for id in range (1, Q.num_cols()):\n",
    "            gamma.set(id, 1.0)\n",
    "        \n",
    "        gamma = gamma.scale(1.0 / (ind * ind))\n",
    "        xbar = state.get_x_bar()\n",
    "\n",
    "        # update the average\n",
    "        state.setXbar(update_incremental_mean(xbar, x, ind))\n",
    "        xbar = state.get_x_bar()\n",
    "\n",
    "        # for the update remove the average\n",
    "        x = x.minus(xbar).transpose()\n",
    "\n",
    "        # update the predictor\n",
    "        y = Q.mult(x)\n",
    "\n",
    "        # prepare new state\n",
    "        m = Q.numRows()\n",
    "        n = Q.num_cols()\n",
    "        gamy = gamma.element_multiply(y)\n",
    "        b = Q.extract_vector(0).scale(y.get(0))\n",
    "        A = np.array(m,n)\n",
    "    \n",
    "        A.setColumn(0, 0,\n",
    "                (Q.extract_vector(0)\n",
    "                        .minus(b.scale(gamy.get(0))))\n",
    "                        .getDDRM()\n",
    "                        .data)\n",
    "        for i in range(1, n):\n",
    "            b = b.plus(Q.extract_vector(i).scale(y.get(i)))\n",
    "            A.setColumn(i, 0,\n",
    "                    (Q.extract_vector(i)\n",
    "                            .minus(b.scale(gamy.get(i))))\n",
    "                            .getDDRM()\n",
    "                            .data)\n",
    "\n",
    "        A = A.plus(x.mult(gamy.transpose()))\n",
    "        decay = ((gamma.minus(1.0)).scale(-1.0)).element_multiply(lambdaVal)\n",
    "        increment = gamma.element_multiply(y).element_multiply(y)\n",
    "        lambdaVal = increment.plus(decay)\n",
    "\n",
    "        # wrap return values\n",
    "        state.set_lambda_val(lambdaVal)\n",
    "        state.set_q_val(A)\n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beyond i.i.d\n",
    "\n",
    "The key idea of __covariate shift adaptation__ is to (softly) __choose informative training samples__ in a systematic way, by considering the __importance__ of each __training sample__ in the __prediction of test output values__, namely the ratio\n",
    "\n",
    "$$ \\frac{p_{te}(x_i^{tr})}{p_{tr}(x_i^{tr})}$$\n",
    "\n",
    "Basically, __the expectation__ of a function $f$ (i.e. regression) over $x_{te}$ can be computed by the __importance-weighted expectation__ of the function over $x_{tr}$. \n",
    "\n",
    "Thus, the __difference of distributions__ can be systematically __adjusted__ by importance weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real-world application [Axenie et al., ECML PKDD 2019]\n",
    "![](coke.png)\n",
    "\n",
    "#### Multi-class classification task (fault identification in predictive maintenance [Axenie et al., 2019])\n",
    "\n",
    "Goal: identify faults in the production line by querying the eigenvalues and eigenvectors to extract the normal and faulty operation configuration prior to a multi-class classifier.\n",
    "\n",
    "The datastream:\n",
    "\n",
    "- 2M incoming events at 40 kHz.\n",
    "- eigenvalues of the input $X$ are close to the class labels (i.e. $1, 2, ..., d$)\n",
    "- eigenvectors are close to the canonical basis of $R^d$, where $d$ is the number of principal components to extract class number for the multi-class classification task (i.e. 10 classes, 9 faults and 1 normal).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental setup\n",
    "![](experimental-setup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance analysis\n",
    "![](throughput-analysis.png)\n",
    "![](latency-analysis.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy\n",
    "![](performance-vals.png)\n",
    "![](accuracy-analysis.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
